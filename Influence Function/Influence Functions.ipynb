{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're gonna have some basic setup before we get to the focus.\n",
        "Here we define the models and make a couple auxiliary functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Defining linear and logistic models\n",
        "def linear(x, slope, intercept):\n",
        "    return slope*x + intercept\n",
        "\n",
        "def logit(p):\n",
        "    return np.log(p/(1-p))\n",
        "\n",
        "def inv_logit(alpha):\n",
        "    return np.exp(alpha)/(np.exp(alpha)+1)\n",
        "\n",
        "def logistic(x, beta_0, beta_1):\n",
        "    return inv_logit(linear(x, beta_0, beta_1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the gradient of vector feld f at v\n",
        "def gradient(f, v):\n",
        "    differential_length: float = 2**-16 # this can be adjusted\n",
        "\n",
        "    n = len(v)\n",
        "    grad = np.zeros(n)\n",
        "\n",
        "    # I wonder if numpy has a better way to do this?\n",
        "    for i in range(len(v)):\n",
        "        d = np.zeros(n)\n",
        "        d[i] = differential_length\n",
        "        directional_derivative = (f(v+d/2) - f(v-d/2))/differential_length\n",
        "        grad[i] = directional_derivative\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And here are parameters to decide what model we're using. **Edit this section** if you want to test with a different model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# These can be changed to run different tests\n",
        "# You need to change the loss function below also\n",
        "model = linear\n",
        "step_size = 2**-12\n",
        "epoch_count = 2**14\n",
        "\n",
        "# This gets the parameter count of the model\n",
        "# (Which is one less than the number of arguments that model takes, since one of the arguments is the input vector)\n",
        "import inspect\n",
        "param_count = len(inspect.signature(model).parameters) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining the loss function\n",
        "def squared_error(params, x, y):\n",
        "    squared_error = (model(x, *params) - y)**2\n",
        "    return squared_error\n",
        "\n",
        "def log_loss(params, x, y):\n",
        "    prob = model(x, *params)\n",
        "    if y:\n",
        "        return -np.log(prob)\n",
        "    else:\n",
        "        return -np.log(1-prob)\n",
        "\n",
        "def mean_loss(pointwise_loss):\n",
        "    def f(params, xdata, ydata, weights = None):\n",
        "        n = len(xdata)\n",
        "        if weights is None:\n",
        "            weights = [1] * n\n",
        "        losses = [pointwise_loss(params, xdata[i], ydata[i])*weights[i] for i in range(n)]\n",
        "        return sum(losses)/n\n",
        "    return f\n",
        "\n",
        "\n",
        "loss = mean_loss(squared_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# These determine the training data\n",
        "xdata = np.array([0, 1, 2])\n",
        "ydata = np.array([0, 1.5, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's the code for training the model using gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gradient_descent(params, xdata, ydata, weights, loss = loss):\n",
        "    f = lambda theta: loss(theta, xdata, ydata, weights)\n",
        "    for _ in range(epoch_count):\n",
        "        grad = gradient(f, params)\n",
        "        params -= grad*step_size\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the response function.\n",
        "The response function calculates the parameters which result from retraining with an additional datapoint *new* that is weighted by some amount *epsilon*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Influence functions are defined as the first-order Taylor approximation of this around epsilon = 0\n",
        "# Or maybe the derivative if it's more convenient to subtract off the value at 0, I think I've seen both definitions\n",
        "def response_function(new_data, epsilon, xdata, ydata):\n",
        "    newxdata = np.concatenate((np.array([new_data[0]]), xdata))\n",
        "    newydata = np.concatenate((np.array([new_data[1]]), ydata))\n",
        "    weights = np.array([epsilon] + [1 for _ in xdata])\n",
        "    params = gradient_descent(np.zeros(param_count), newxdata, newydata, weights)\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's the influence function (on the parameters). It's the derivative of the response function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def influence(z, epsilon):\n",
        "    a = response_function(z, epsilon/2, xdata, ydata)\n",
        "    b = response_function(z, -epsilon/2, xdata, ydata)\n",
        "    return (a-b)/epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This makes a plot. It will need considerable editing if you're doing a different testâ€”I should probably set up something more versatile later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "gradient_descent() missing 1 required positional argument: 'loss'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\april\\School\\Reed-Thesis\\Influence Function\\Influence Functions.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/april/School/Reed-Thesis/Influence%20Function/Influence%20Functions.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m z2 \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m,\u001b[39m1.5\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/april/School/Reed-Thesis/Influence%20Function/Influence%20Functions.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m z3 \u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/april/School/Reed-Thesis/Influence%20Function/Influence%20Functions.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y \u001b[39m=\u001b[39m response_function(z1, \u001b[39m0\u001b[39;49m, xdata, ydata)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/april/School/Reed-Thesis/Influence%20Function/Influence%20Functions.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInitial model: \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/april/School/Reed-Thesis/Influence%20Function/Influence%20Functions.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m influence1 \u001b[39m=\u001b[39m influence(z1, epsilon)\n",
            "\u001b[1;32mc:\\Users\\april\\School\\Reed-Thesis\\Influence Function\\Influence Functions.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/april/School/Reed-Thesis/Influence%20Function/Influence%20Functions.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m newydata \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((np\u001b[39m.\u001b[39marray([new_data[\u001b[39m1\u001b[39m]]), ydata))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/april/School/Reed-Thesis/Influence%20Function/Influence%20Functions.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([epsilon] \u001b[39m+\u001b[39m [\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m xdata])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/april/School/Reed-Thesis/Influence%20Function/Influence%20Functions.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m params \u001b[39m=\u001b[39m gradient_descent(np\u001b[39m.\u001b[39;49mzeros(param_count), newxdata, newydata, weights)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/april/School/Reed-Thesis/Influence%20Function/Influence%20Functions.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mreturn\u001b[39;00m params\n",
            "\u001b[1;31mTypeError\u001b[0m: gradient_descent() missing 1 required positional argument: 'loss'"
          ]
        }
      ],
      "source": [
        "epsilon = 0.001\n",
        "delta = 1\n",
        "z1 = (0,0)\n",
        "z2 = (1,1.5)\n",
        "z3 = (2,2)\n",
        "y = response_function(z1, 0, xdata, ydata)\n",
        "print(f'Initial model: {y}')\n",
        "influence1 = influence(z1, epsilon)\n",
        "influence2 = influence(z2, epsilon)\n",
        "influence3 = influence(z3, epsilon)\n",
        "linspace = np.linspace(xdata.min(), xdata.max(), 500)\n",
        "\n",
        "x1 = y + influence1*delta\n",
        "x2 = y + influence2*delta\n",
        "x3 = y + influence3*delta\n",
        "\n",
        "print(influence1)\n",
        "print(influence2)\n",
        "print(influence3)\n",
        "\n",
        "plt.plot(linspace, model(linspace, *y), color='black',linewidth=2, label=\"Initial Model\")\n",
        "plt.plot(linspace, model(linspace, *x1), color='red',linewidth=1, label=f\"$\\hat\\\\theta + \\mathcal{{I}}(0,0)$\")\n",
        "plt.plot(linspace, model(linspace, *x2), color='orange',linewidth=1, label=f\"$\\hat\\\\theta + \\mathcal{{I}}(1,1.5)$\")\n",
        "plt.plot(linspace, model(linspace, *x3), color='blue',linewidth=1, label=f\"$\\hat\\\\theta + \\mathcal{{I}}(2,2)$\")\n",
        "plt.scatter(xdata,ydata)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-0.13120544192640377\n",
            "Cost Influence of (0, 0) on (0, 0):\n",
            "-0.04716392861213299\n",
            "Cost Influence of (0, 0) on (1, 1.5):\n",
            "0.0374848913334374\n",
            "Cost Influence of (0, 0) on (2, 2):\n",
            "0.009495262163364715\n",
            "Cost Influence of (1, 1.5) on (0, 0):\n",
            "0.03702740217143974\n",
            "Cost Influence of (1, 1.5) on (1, 1.5):\n",
            "-0.07386963571736015\n",
            "Cost Influence of (1, 1.5) on (2, 2):\n",
            "0.03685298276811481\n",
            "Cost Influence of (2, 2) on (0, 0):\n",
            "0.009103179214047272\n",
            "Cost Influence of (2, 2) on (1, 1.5):\n",
            "0.03687138332158968\n",
            "Cost Influence of (2, 2) on (2, 2):\n",
            "-0.04580656857786415\n"
          ]
        }
      ],
      "source": [
        "# Calculates the influence of z1 on the loss on z2\n",
        "# This is a directional derivative of loss on z2 in the direction of the influence of z1 \n",
        "def loss_influence(params, z1, z2, epsilon):\n",
        "    point_loss = lambda theta: loss(theta, z2[0], z2[1])\n",
        "    grad = gradient(point_loss, params)\n",
        "    return np.dot(grad, influence(z1, epsilon))\n",
        "\n",
        "print(loss_influence(y, (0,0), (1,0), epsilon))\n",
        "\n",
        "data = [(0, 0), (1, 1.5), (2, 2)]\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        print(f'Cost Influence of {data[i]} on {data[j]}:')\n",
        "        print(loss_influence(y, data[i], data[j], epsilon))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
